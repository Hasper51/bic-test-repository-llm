#!/usr/bin/env python3
"""
–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ FastAPI OpenRouter —Å–µ—Ä–≤–µ—Ä–∞
10 —Ç–µ—Å—Ç–æ–≤: 5 –¥–ª—è /generate + 5 –¥–ª—è /benchmark
"""

import requests
import json
import time
import statistics
import csv
from datetime import datetime
import os

BASE_URL = "http://localhost:8000"

# –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
TEST_PROMPTS = [
    "–ö—Ä–∞—Ç–∫–æ, —á—Ç–æ —Ç–∞–∫–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç?",
    "–ù–∞–ø–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–æ–µ —Å—Ç–∏—Ö–æ—Ç–≤–æ—Ä–µ–Ω–∏–µ –æ –≤–µ—Å–Ω–µ.",
    "–†–∞—Å—Å—Å–∫–∞–∂–∏ –∫–æ—Ä–æ—Ç–∫–∏–π –∞–Ω–µ–∫–¥–æ—Ç –ø—Ä–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–≤.",
]

MODELS = [
    "deepseek/deepseek-chat-v3.1:free",
    "z-ai/glm-4.5-air:free",
    "moonshotai/kimi-k2:free",
]


def measure_request_time(func):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""

    def wrapper(*args, **kwargs):
        start = time.time()

        result = func(*args, **kwargs)
        end = time.time()
        return result, end - start

    return wrapper


@measure_request_time
def test_generate_normal(prompt, model, max_tokens=256):
    """–¢–µ—Å—Ç –æ–±—ã—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
    data = {"prompt": prompt, "model": model, "max_tokens": max_tokens, "stream": False}

    response = requests.post(
        f"{BASE_URL}/generate",
        json=data,
        headers={"Content-Type": "application/json"},
        timeout=60,
    )

    return {
        "status_code": response.status_code,
        "success": response.status_code == 200,
        "response_data": response.json() if response.status_code == 200 else None,
        "error": response.text if response.status_code != 200 else None,
    }


@measure_request_time
def test_generate_stream(prompt, model, max_tokens=256):
    """–¢–µ—Å—Ç SSE —Å—Ç—Ä–∏–º–∏–Ω–≥–∞"""
    data = {"prompt": prompt, "model": model, "max_tokens": max_tokens, "stream": True}

    response = requests.post(
        f"{BASE_URL}/generate",
        json=data,
        headers={"Content-Type": "application/json"},
        timeout=60,
        stream=True,
    )

    # –°–æ–±–∏—Ä–∞–µ–º –≤–µ—Å—å —Å—Ç—Ä–∏–º
    chunks = []
    if response.status_code == 200:
        for line in response.iter_lines():
            if line:
                line_str = line.decode("utf-8")
                if line_str.startswith("data: "):
                    try:
                        data_obj = json.loads(line_str[6:])
                        if "content" in data_obj:
                            chunks.append(data_obj["content"])
                        elif data_obj.get("done"):
                            break
                    except json.JSONDecodeError:
                        continue

    return {
        "status_code": response.status_code,
        "success": response.status_code == 200,
        "chunks_received": len(chunks),
        "total_content": "".join(chunks) if chunks else "",
        "error": response.text if response.status_code != 200 else None,
    }


@measure_request_time
def test_benchmark_normal(model, runs, prompts_file):
    """–¢–µ—Å—Ç –æ–±—ã—á–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞"""
    with open(prompts_file, "r", encoding="utf-8") as f:
        prompts_content = f.read()

    files = {"prompt_file": ("test_prompts.txt", prompts_content, "text/plain")}
    data = {"model": model, "runs": str(runs), "visualize": "false"}

    response = requests.post(
        f"{BASE_URL}/benchmark", files=files, data=data, timeout=300
    )

    return {
        "status_code": response.status_code,
        "success": response.status_code == 200,
        "response_data": response.json() if response.status_code == 200 else None,
        "error": response.text if response.status_code != 200 else None,
    }


@measure_request_time
def test_benchmark_visualized(model, runs, prompts_file):
    """–¢–µ—Å—Ç –±–µ–Ω—á–º–∞—Ä–∫–∞ —Å HTML –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π"""
    with open(prompts_file, "r", encoding="utf-8") as f:
        prompts_content = f.read()

    files = {"prompt_file": ("test_prompts.txt", prompts_content, "text/plain")}
    data = {"model": model, "runs": str(runs), "visualize": "true"}

    response = requests.post(
        f"{BASE_URL}/benchmark", files=files, data=data, timeout=300
    )

    return {
        "status_code": response.status_code,
        "success": response.status_code == 200,
        "html_length": len(response.text) if response.status_code == 200 else 0,
        "is_html": response.headers.get("content-type", "").startswith("text/html"),
        "error": response.text[:500] if response.status_code != 200 else None,
    }


def create_test_prompts_file():
    """–°–æ–∑–¥–∞–µ—Ç —Ñ–∞–π–ª —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –ø—Ä–æ–º–ø—Ç–∞–º–∏"""
    filename = "test_prompts.txt"
    with open(filename, "w", encoding="utf-8") as f:
        f.write("\n".join(TEST_PROMPTS))
    return filename


def run_generate_tests():
    """–ó–∞–ø—É—Å–∫ 5 —Ç–µ—Å—Ç–æ–≤ –¥–ª—è /generate"""
    print("=" * 60)
    print("–¢–ï–°–¢–´ /generate (5 —Ç–µ—Å—Ç–æ–≤)")
    print("=" * 60)

    results = []

    # –¢–µ—Å—Ç 1: –û–±—ã—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø–µ—Ä–≤–æ–π –º–æ–¥–µ–ª—å—é
    print("\n1. –¢–µ—Å—Ç –æ–±—ã—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (deepseek)")
    result, latency = test_generate_normal(TEST_PROMPTS[0], MODELS[0])
    results.append(
        {
            "test_name": "generate_normal_deepseek",
            "model": MODELS[0],
            "latency": latency,
            "success": result["success"],
            "tokens_used": (
                result["response_data"]["tokens_used"] if result["success"] else 0
            ),
        }
    )
    print(f"  –£—Å–ø–µ—Ö: {result['success']}")
    print(f"  –í—Ä–µ–º—è: {latency:.3f}s")
    print(f"  –û—à–∏–±–∫–∞: {result['error']}" if not result["success"] else "")
    if result["success"]:
        print(f"  –¢–æ–∫–µ–Ω—ã: {result['response_data']['tokens_used']}")
        print(f"  –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {len(result['response_data']['response'])}")

    # –¢–µ—Å—Ç 2: SSE —Å—Ç—Ä–∏–º–∏–Ω–≥ —Å –ø–µ—Ä–≤–æ–π –º–æ–¥–µ–ª—å—é
    print("\n2. –¢–µ—Å—Ç SSE —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ (deepseek)")
    result, latency = test_generate_stream(TEST_PROMPTS[0], MODELS[0])
    results.append(
        {
            "test_name": "generate_stream_deepseek",
            "model": MODELS[0],
            "latency": latency,
            "success": result["success"],
            "chunks_received": result["chunks_received"] if result["success"] else 0,
        }
    )
    print(f"  –£—Å–ø–µ—Ö: {result['success']}")
    print(f"  –í—Ä–µ–º—è: {latency:.3f}s")
    if result["success"]:
        print(f"  –ß–∞–Ω–∫–æ–≤ –ø–æ–ª—É—á–µ–Ω–æ: {result['chunks_received']}")
        print(f"  –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {len(result['total_content'])}")

    # –¢–µ—Å—Ç 3: –û–±—ã—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–æ –≤—Ç–æ—Ä–æ–π –º–æ–¥–µ–ª—å—é
    print("\n3. –¢–µ—Å—Ç –æ–±—ã—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (glm-4.5)")
    result, latency = test_generate_normal(TEST_PROMPTS[0], MODELS[1])
    results.append(
        {
            "test_name": "generate_normal_glm",
            "model": MODELS[1],
            "latency": latency,
            "success": result["success"],
            "tokens_used": (
                result["response_data"]["tokens_used"] if result["success"] else 0
            ),
        }
    )
    print(f"  –£—Å–ø–µ—Ö: {result['success']}")
    print(f"  –í—Ä–µ–º—è: {latency:.3f}s")
    if result["success"]:
        print(f"  –¢–æ–∫–µ–Ω—ã: {result['response_data']['tokens_used']}")

    # –¢–µ—Å—Ç 4: SSE —Å—Ç—Ä–∏–º–∏–Ω–≥ —Å–æ –≤—Ç–æ—Ä–æ–π –º–æ–¥–µ–ª—å—é
    print("\n4. –¢–µ—Å—Ç SSE —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ (glm-4.5)")
    result, latency = test_generate_stream(TEST_PROMPTS[0], MODELS[1])
    results.append(
        {
            "test_name": "generate_stream_glm",
            "model": MODELS[1],
            "latency": latency,
            "success": result["success"],
            "chunks_received": result["chunks_received"] if result["success"] else 0,
        }
    )
    print(f"  –£—Å–ø–µ—Ö: {result['success']}")
    print(f"  –í—Ä–µ–º—è: {latency:.3f}s")
    if result["success"]:
        print(f"  –ß–∞–Ω–∫–æ–≤ –ø–æ–ª—É—á–µ–Ω–æ: {result['chunks_received']}")

    # –¢–µ—Å—Ç 5: –û–±—ã—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å —Ç—Ä–µ—Ç—å–µ–π –º–æ–¥–µ–ª—å—é
    print("\n5. –¢–µ—Å—Ç –æ–±—ã—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏(kimi)")
    result, latency = test_generate_normal(TEST_PROMPTS[0], MODELS[2])
    results.append(
        {
            "test_name": "generate_normal_kimi_512",
            "model": MODELS[2],
            "latency": latency,
            "success": result["success"],
            "tokens_used": (
                result["response_data"]["tokens_used"] if result["success"] else 0
            ),
        }
    )
    print(f"  –£—Å–ø–µ—Ö: {result['success']}")
    print(f"  –í—Ä–µ–º—è: {latency:.3f}s")
    if result["success"]:
        print(f"  –¢–æ–∫–µ–Ω—ã: {result['response_data']['tokens_used']}")

    return results


def run_benchmark_tests():
    """–ó–∞–ø—É—Å–∫ 5 —Ç–µ—Å—Ç–æ–≤ –¥–ª—è /benchmark"""
    print("\n" + "=" * 60)
    print("–¢–ï–°–¢–´ /benchmark (5 —Ç–µ—Å—Ç–æ–≤)")
    print("=" * 60)

    results = []
    prompts_file = create_test_prompts_file()

    # –¢–µ—Å—Ç 1: –ë–µ–Ω—á–º–∞—Ä–∫ deepseek, 1 –ø—Ä–æ–≥–æ–Ω
    print("\n1. –ë–µ–Ω—á–º–∞—Ä–∫ deepseek (runs=1)")
    result, latency = test_benchmark_normal(MODELS[0], 1, prompts_file)
    results.append(
        {
            "test_name": "benchmark_deepseek_2runs",
            "model": MODELS[0],
            "latency": latency,
            "success": result["success"],
            "runs": 2,
            "total_prompts": (
                result["response_data"]["total_prompts"] if result["success"] else 0
            ),
        }
    )
    print(f"  –£—Å–ø–µ—Ö: {result['success']}")
    print(f"  –í—Ä–µ–º—è: {latency:.3f}s")
    if result["success"]:
        data = result["response_data"]
        print(f"  –ü—Ä–æ–º–ø—Ç–æ–≤: {data['total_prompts']}")
        print(f"  –°—Ä–µ–¥–Ω—è—è –∑–∞–¥–µ—Ä–∂–∫–∞: {data['latency_stats']['avg']}s")
        print(f"  CSV —Ñ–∞–π–ª: {data['results_file']}")

    # –¢–µ—Å—Ç 2: –ë–µ–Ω—á–º–∞—Ä–∫ glm-4.5, 2 –ø—Ä–æ–≥–æ–Ω–∞
    print("\n2. –ë–µ–Ω—á–º–∞—Ä–∫ glm-4.5 (runs=2)")
    result, latency = test_benchmark_normal(MODELS[1], 2, prompts_file)
    results.append(
        {
            "test_name": "benchmark_glm_3runs",
            "model": MODELS[1],
            "latency": latency,
            "success": result["success"],
            "runs": 3,
            "total_prompts": (
                result["response_data"]["total_prompts"] if result["success"] else 0
            ),
        }
    )
    print(f"  –£—Å–ø–µ—Ö: {result['success']}")
    print(f"  –í—Ä–µ–º—è: {latency:.3f}s")
    if result["success"]:
        data = result["response_data"]
        print(f"  –ü—Ä–æ–º–ø—Ç–æ–≤: {data['total_prompts']}")
        print(f"  –°—Ä–µ–¥–Ω—è—è –∑–∞–¥–µ—Ä–∂–∫–∞: {data['latency_stats']['avg']}s")

    # –¢–µ—Å—Ç 3: –ë–µ–Ω—á–º–∞—Ä–∫ kimi, 2 –ø—Ä–æ–≥–æ–Ω–∞ —Å HTML –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π
    print("\n3. –ë–µ–Ω—á–º–∞—Ä–∫ kimi —Å HTML –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π (runs=1)")
    result, latency = test_benchmark_visualized(MODELS[2], 1, prompts_file)
    results.append(
        {
            "test_name": "benchmark_kimi_html",
            "model": MODELS[2],
            "latency": latency,
            "success": result["success"],
            "runs": 2,
            "html_generated": result["is_html"] if result["success"] else False,
        }
    )
    print(f"  –£—Å–ø–µ—Ö: {result['success']}")
    print(f"  –í—Ä–µ–º—è: {latency:.3f}s")
    if result["success"]:
        print(f"  HTML —Ä–∞–∑–º–µ—Ä: {result['html_length']} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"  –≠—Ç–æ HTML: {result['is_html']}")

    # –¢–µ—Å—Ç 4: –ë–æ–ª—å—à–æ–π –±–µ–Ω—á–º–∞—Ä–∫ deepseek, 3 –ø—Ä–æ–≥–æ–Ω–∞
    print("\n4. –ë–æ–ª—å—à–æ–π –±–µ–Ω—á–º–∞—Ä–∫ deepseek (runs=3)")
    result, latency = test_benchmark_normal(MODELS[0], 3, prompts_file)
    results.append(
        {
            "test_name": "benchmark_deepseek_4runs",
            "model": MODELS[0],
            "latency": latency,
            "success": result["success"],
            "runs": 4,
            "total_prompts": (
                result["response_data"]["total_prompts"] if result["success"] else 0
            ),
        }
    )
    print(f"  –£—Å–ø–µ—Ö: {result['success']}")
    print(f"  –í—Ä–µ–º—è: {latency:.3f}s")
    if result["success"]:
        data = result["response_data"]
        print(f"  –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {data['total_prompts'] * data['runs']}")
        print(f"  –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {data['latency_stats']['std_dev']}s")

    # –¢–µ—Å—Ç 5: –ë–µ–Ω—á–º–∞—Ä–∫ glm-4.5 —Å HTML –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π, 2 –ø—Ä–æ–≥–æ–Ω–∞
    print("\n5. –ë–µ–Ω—á–º–∞—Ä–∫ glm-4.5 —Å HTML (runs=2)")
    result, latency = test_benchmark_visualized(MODELS[1], 2, prompts_file)
    results.append(
        {
            "test_name": "benchmark_glm_3runs_html",
            "model": MODELS[1],
            "latency": latency,
            "success": result["success"],
            "runs": 3,
            "html_generated": result["is_html"] if result["success"] else False,
        }
    )
    print(f"  –£—Å–ø–µ—Ö: {result['success']}")
    print(f"  –í—Ä–µ–º—è: {latency:.3f}s")
    if result["success"]:
        print(f"  HTML —Ä–∞–∑–º–µ—Ä: {result['html_length']} —Å–∏–º–≤–æ–ª–æ–≤")

    return results


def create_comparison_table(generate_results, benchmark_results):
    """–°–æ–∑–¥–∞–µ—Ç —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –º–æ–¥–µ–ª–µ–π"""
    print("\n" + "=" * 80)
    print("–°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê –ú–û–î–ï–õ–ï–ô")
    print("=" * 80)

    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–æ–¥–µ–ª—è–º
    model_stats = {}

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    for result in generate_results:
        model = result["model"]
        if model not in model_stats:
            model_stats[model] = {"generate_latencies": [], "benchmark_latencies": []}
        if result["success"]:
            model_stats[model]["generate_latencies"].append(result["latency"])

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–æ–≤
    for result in benchmark_results:
        model = result["model"]
        if model not in model_stats:
            model_stats[model] = {"generate_latencies": [], "benchmark_latencies": []}
        if result["success"]:
            model_stats[model]["benchmark_latencies"].append(result["latency"])

    # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É
    print(
        f"{'–ú–æ–¥–µ–ª—å':<30} {'Generate Avg':<15} {'Generate StdDev':<15} {'Benchmark Avg':<15} {'Benchmark StdDev':<15}"
    )
    print("-" * 90)

    comparison_data = []

    for model, stats in model_stats.items():
        gen_latencies = stats["generate_latencies"]
        bench_latencies = stats["benchmark_latencies"]

        gen_avg = statistics.mean(gen_latencies) if gen_latencies else 0
        gen_std = statistics.stdev(gen_latencies) if len(gen_latencies) > 1 else 0

        bench_avg = statistics.mean(bench_latencies) if bench_latencies else 0
        bench_std = statistics.stdev(bench_latencies) if len(bench_latencies) > 1 else 0

        model_short = model.split("/")[-1]  # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏

        print(
            f"{model_short:<30} {gen_avg:<15.3f} {gen_std:<15.3f} {bench_avg:<15.3f} {bench_std:<15.3f}"
        )

        comparison_data.append(
            {
                "model": model,
                "generate_avg_latency": round(gen_avg, 3),
                "generate_std_dev": round(gen_std, 3),
                "benchmark_avg_latency": round(bench_avg, 3),
                "benchmark_std_dev": round(bench_std, 3),
                "generate_tests": len(gen_latencies),
                "benchmark_tests": len(bench_latencies),
            }
        )

    return comparison_data


def save_results_to_csv(generate_results, benchmark_results, comparison_data):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ CSV —Ñ–∞–π–ª—ã"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    with open(
        f"test_results_generate_{timestamp}.csv", "w", newline="", encoding="utf-8"
    ) as f:
        if generate_results:
            # Build ordered union of all keys across all result dicts to avoid missing/extra fields
            fieldnames = []
            for row in generate_results:
                for k in row.keys():
                    if k not in fieldnames:
                        fieldnames.append(k)
            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction="ignore")
            writer.writeheader()
            writer.writerows(generate_results)

    # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞
    with open(
        f"test_results_benchmark_{timestamp}.csv", "w", newline="", encoding="utf-8"
    ) as f:
        if benchmark_results:
            # Build ordered union of all keys across all result dicts
            fieldnames = []
            for row in benchmark_results:
                for k in row.keys():
                    if k not in fieldnames:
                        fieldnames.append(k)
            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction="ignore")
            writer.writeheader()
            writer.writerows(benchmark_results)

    # –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    with open(
        f"model_comparison_{timestamp}.csv", "w", newline="", encoding="utf-8"
    ) as f:
        if comparison_data:
            # Build ordered union of all keys across all comparison rows
            fieldnames = []
            for row in comparison_data:
                for k in row.keys():
                    if k not in fieldnames:
                        fieldnames.append(k)
            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction="ignore")
            writer.writeheader()
            writer.writerows(comparison_data)

    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
    print(f"  - test_results_generate_{timestamp}.csv")
    print(f"  - test_results_benchmark_{timestamp}.csv")
    print(f"  - model_comparison_{timestamp}.csv")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üöÄ –ö–û–ú–ü–õ–ï–ö–°–ù–û–ï –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï API")
    print("=" * 80)

    start_time = time.time()

    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã
    generate_results = run_generate_tests()
    benchmark_results = run_benchmark_tests()

    # –°–æ–∑–¥–∞–µ–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
    comparison_data = create_comparison_table(generate_results, benchmark_results)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    save_results_to_csv(generate_results, benchmark_results, comparison_data)

    total_time = time.time() - start_time

    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "=" * 80)
    print("–ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("=" * 80)

    generate_success = sum(1 for r in generate_results if r["success"])
    benchmark_success = sum(1 for r in benchmark_results if r["success"])

    print(f"–¢–µ—Å—Ç—ã /generate:   {generate_success}/5 —É—Å–ø–µ—à–Ω—ã—Ö")
    print(f"–¢–µ—Å—Ç—ã /benchmark:  {benchmark_success}/5 —É—Å–ø–µ—à–Ω—ã—Ö")
    print(f"–û–±—â–µ–µ –≤—Ä–µ–º—è:       {total_time:.1f} —Å–µ–∫—É–Ω–¥")
    print(f"–í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤:      10")
    print(f"–£—Å–ø–µ—à–Ω–æ—Å—Ç—å:        {(generate_success + benchmark_success)/10*100:.1f}%")

    # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
    if os.path.exists("test_prompts.txt"):
        os.remove("test_prompts.txt")

    print("\n‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")


if __name__ == "__main__":
    main()
